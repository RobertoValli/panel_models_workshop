\documentclass[hidelinks]{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}}}
\makeatother
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\makeatletter
\@ifundefined{AddToHook}{}{\AddToHook{package/xcolor/after}{
\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
}}
\makeatother
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}    
\usepackage{csquotes}
\usepackage[style=apa, backend=biber, maxcitenames=3]{biblatex}


\DeclareLanguageMapping{american}{american-apa}

\AtEveryBibitem{\clearfield{labelmonth}}

\addbibresource{refs_workshop.bib}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{qtree}
%\usepackage{makecell}

\usepackage[margin = 2.5cm]{geometry}
\usepackage{lmodern}
%\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{rotating}
\usepackage{caption} 
\usepackage{subcaption}
%\captionsetup[table]{skip=10pt}

\renewcommand{\baselinestretch}{1.2}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0em}

\usepackage[breaklinks]{hyperref}

\author{
	Roberto Valli\thanks{PhD candidate, International Conflict Research, ETH Zurich: roberto.valli@icr.gess.ethz.ch} 
}
\title{
	 Recent Developments in Methods for Causal Inference with Cross-Sectional Time-Series Data\thanks{Find the workshop's GitHub repo at \url{https://github.com/RobertoValli/panel_models_workshop}.}\\[1em]
	 \textit{Workshop Notes}
}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
## [1] 0
\end{verbatim}
\end{kframe}
\end{knitrout}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\maketitle

\section{Introduction}

Political science is often characterized by research questions that cannot be studied experimentally because ethical or practical reasons prevent treatment manipulation. The impossibility of using true randomization in polisci research makes all the more important to find alternative ways to estimate causal effects of policies and other interventions. These notes discuss recent developments in a specific family of statistical methods that exploit within and across-unit variation over time to approximate estimates of causal effects. 

The goal of these notes in to tie together a recent literature that highlights the limitations of standard difference-in-differences methods (DiD) when it comes to estimating causal effects in complex settings that violate its basic assumptions. The interesting thing about this literature is that not only it develops extremely fast, but it accompanies a harsh critique of common modelling choices to many solutions, practical advice, and R packages to implement more suitable models. However, the fast-moving and technical nature of the literature can be confusing. This is why I wrote these notes, to guide applied researchers to the right references and modelling choices without having to first read everything under the sun.

Note that this document is meant to be a practical guide for advanced researchers with prior knowledge of statistics, basic causal inference designs and programming in \texttt{R}. It is by no means a technical paper, nor an introduction to any of these subjects. Also, the focus of this paper is on the different methods used to statistically estimate average treatment effects and not on the related ways to compute uncertainty measures. Applied scholars need to learn on these through the original methods papers and the related R package documentation. Moreover, this note does not cover related topics, such as developments of DiD estimators that drop standard sampling assumptions for design-based inference and the literature on event study models.

The notes proceed as follows. First, I give a brief overview of causal inference methods with cross-sectional time-series data (CSTS) and its limitations. I start from the standard DiD model and its estimators, and walk through its critique in the literature. Second, I present a heuristic decision model to select the right CSTS method based on one's data. Third, I provide a description of various methods, their assumptions and implementation in \texttt{R}.

\section{Causality and Cross-Sectional Time-Series Data}

The problem of estimating causal effects can be thought of as the challenge of predicting counterfactual outcomes. The impossibility of observing the outcome of a process if something (the treatment condition) had been different is often referred to as the \textit{fundamental problem of causal inference}. The Neymar-Rubin counterfactual outcome framework formalizes one way of thinking about causality in terms of counterfactuals. 

\subsection{The Classic DiD Setup as a Motivating Example}

Consider an outcome of interest $Y_{it}$ that is observed for individual $i$ at time $t$. Also, consider a binary treatment $D_{it} \in \{ 0, 1 \}$. The individual causal effect of $D$ on $Y$ can be defined as 
$$\text{ICE} = Y_{it}(1) - Y_{it}(0)$$

However, since only one of the two terms is ever observed in reality, statistical methods of causal inference find ways to estimate \textit{average causal effects} by predicting the counterfactual outcome under the unobserved treatment condition. In fact, while applied statistics are often divided between causal inference and prediction tasks, prediction is very much at the heart of causal inference methods, although with a strong focus on bias reduction.

Let's consider the classic two-period DiD setup and how it can be thought of as a prediction exercise. The outcome is assumed to be a continuous variable $Y_{it}$ observed over two periods $t=1, 2$, whereas the treatment is assumed to be a dichotomous variable $D_{it} = 0, 1$. The observed value of the outcome can be expressed as:
$$ Y_{it} = Y_{it}(1)D_{it} + Y_{it}(0)D_{it} $$
The major contribution of the classic DiD literature \parencite[e.g.,][]{Angrist2009} is to demonstrate that under SUTVA\footnote{The \textit{stable unit treatment value assumption} is a set of assumptions such that: i) units composition does not change over time, ii) no treatment heterogeneity exists, iii) no treatment spillovers take place.} assumptions the average treatment effect on the treated (ATT) can be defined as
$$ \tau = E(Y_{it} (1) - Y_{it}(0) | D_{it} = 1) $$

Under the additional assumption of so-called parallel trends, meaning that the individual \textit{change} in outcome under control between $t = 1, t= 2$ is independent of treatment status, i.e.
$$ E(Y_{i2} (1) - Y_{i1}(0) | D_{it} = 1) = E(Y_{i2} (1) - Y_{i1}(0) | D_{it} = 0) $$
it is possible to statistically identify the ATT as

$$ \tau = E(Y_{it}(1) - E(Y_{it}(0))$$

The parallel trend assumption is key to understand how DiD estimators predict counterfactuals. In fact, by assuming that the trends in the outcome under control are independent by the realized treatment we justify taking the average change in outcome in the control group $E(Y_{i2} (1) - Y_{i1}(0) | D_{it} = 0)$ as the trend that the treated units would have had, had they not been treated. Then it becomes easy to make a linear prediction of the counterfactual outcomes under no-treatment condition for the treated units with observed data. It is sufficient to add to the outcome of the treated units at $t=1$ the control units' trend
$$ E(Y_{i2} | D_{it} = 1) = E(Y_{i1}(0) | D_{it} = 1) + \left[ E(Y_{i2} (0) - Y_{i1}(0) | D_{it} = 0) \right] $$

\subsection{Causal Inference as a Prediction Problem}

Once the identification of ATTs with DiD methods is thought of as a prediction problem it becomes easier to see the link between DiD and other CSTS estimators such as the synthetic control method (SCM) \parencite{Abadie2010}. In fact, as suggested by \textcite{Roth2022} the two literatures are getting closer to one another. The differences between SCM and DiD are mostly down to data requirement and the way counterfactual outcomes are estimated. In its original formulation, SCM only allows to have one treated unit (usually a state-like political unit), provides a transparent selection of the donor pool, i.e. the units that make up the control group. On the other hand, DiD designs ideally require large population of individuals, where less emphasis is placed on control group selection. Moreover,

Now, in the basic two-period setting there are multiple ways of estimating $\tau$, all of which produce equivalent estimates because difference-in-means are always linear over two periods. One might use a non-parametric approach my estimating a difference in means as done in the classic \textcite{Card1990} paper. Alternatively, one might use a two-way fixed effects regression (TWFE) or a first-difference regression which both have the advantage of producing measures of statistical uncertainty \parencite{Angrist2009}.

However, as soon as one's case of interest goes beyond this simple setup, the assumptions and common estimation methods for the ATT start to be violated. Next, I discuss some of the major problems identified by the literature and touch on the solutions out there.

\section{When the Basic DiD Design Breaks Down}

The econometric literature on TWFEs has produced a substantial number of papers discussing the limitations of the workhorse regression specification in much applied work. Most of these papers focus on the shortcomings of TWFE regressions when used to estimate ATTs in cross-sectional time-series data.

\subsection{Number of Time Periods}

%two or more periods
The simplest violation of the basic DiD setup emerges when units are observed for more than two periods $t=1, 2, \dots, N$. In this case, \textcite{Angrist2009} already note how the first-difference model cannot be considered an unbiased estimator of $\tau$, since taking first-differences over multiple periods introduces autocorrelation of the outcome and violates the independence of the error term. Nevertheless, the same is not true for the two-way fixed effects model, which can be used in multiple-period settings under the assumption of no effect heterogeneity.

\subsection{Treatment Timing}

% staggered adoption
The most significant result in the recent literature on CSTS estimators, and probably the one that contributed the most to its quick development, is the demonstration that the TWFE estimator is a biased estimator of $\tau$ in the common setting of \textit{staggered treatment adoption}. Whenever treated units are not exposed to treatment at the same point in time, but rather adopt treatment in several cohorts, the TWFE estimate of $\hat{\tau}$ is biased towards zero \parencite{GoodmanBacon2021, Callaway2021}. Researchers demonstrated that TWFE estimates are a weighted average of DiD estimates comparing all pairs of treatment statuses and cohorts. The major issue is that some comparisons are undesirable, meaning that they use already-treated cohorts as counterfactual for other treated units. A second problem is that some of the cohort-effects' weights are negative, which creates problem with the interpretation of the aggregate estimate of $\hat{\tau}$. 

There are various solutions to the weighting and comparison problems, which are solved either by setting constraints on group weights, or by explicitly selecting the control group before estimation. The available options are explained below.

\subsection{Violations of Parallel Trends}

% non-parallel trends
The assumption of parallel trends is hard to motivate, and impossible to prove. There are common tests for the existence of pre-treatment trends, like the common event study plots where the outcome is regressed onto lags and leads of the treatment. However, recent studies demonstrated that these tests are underpowered when it comes to rejecting the presence of pre-trends. In fact, there is a growing literature just on this issue, but I won't go into it in this note.

That said, scholars proposed ways to estimate consistent ATTs when parallel trends are valid conditional on a set of covariates \parencite{SantAnna2020}. The authors proposed a family of estimators that have desirable properties when either the outcome model or the covariate set are correctly specified.

% effect heterogeneity

% spillovers and anticipation

% non-absorption state
A treatment is referred to as being in an absorbing state whenever units that are treated cannot revert to being untreated. This is very common in many policy areas, but it does also not apply to many phenomena political scientists want to know the effects of. In fact, there are multiple phenomena in which units can be treated repeatedly over time. There are various problems with repeated and non-absorbing treatments, the major of which is the selection into (repeated) treatment that might bias estimates. However, scholars proposed various methods that allow to deal with these problems, although at the expense of more demanding assumptions.

% continuous treatment
Finally, one might wonder how DiD designs might be extended to continuous treatment settings. While \textcite{Angrist2009} suggest that the type of treatment does not influence the estimand or estimation quantity, \textcite{Callaway2021} demonstrate that continuous treatments make settings with staggered adoption and non-heterogeneous effects extremely hard to handle, as well as deserving a complicated notation. In short, there does not seem to be a set of simple tools and guidelines to be employed in DiDs with continuous outcomes.


\section{Which CSTS Model is Right for You?}

Let's turn to a very applied guide to the right modelling choice depending on one's variation of interest. Note that this typology is strictly limited to binary treatment settings.

\subsection{Number of Treated Units and the Synthetic Control Method}

The first modelling choice is dictated by whether a scholar is interested in the effect of an intervention that treated one (or few) aggregate unit. In this case, the standard synthetic control method (SCM) allows to model the counterfactual outcome of the treated unit flexibly and with extensive diagnostic tools. The idea behind the SCM is to build a model of $Y_{it}(0)\ | \ D=1$ from a pool of untreated units (the ``donor pool''). The model is tuned on the pre-treatment period, and then used to predict the treated units' outcomes under no-treatment status.

The data requirements for ATT estimates with the SCM are i) a sufficiently long pre-treatment period to fit the synthetic control observation, ii) a sufficiently large donor pool of comparable units, iii) the treated unit's outcomes must be within the control units' common support, i.e. the treated unit cannot have outcomes more extreme than the control group. Additionally, SUTVA assumptions hold, which can be hard to assess especially when it comes to the absence of spillovers.
Note that a recent development of SCM by \textcite{BenMichael2021} suggests that by fitting a regularized outcome model it is possible to make valid predictions of counterfactual outcomes even when the treated unit is outside of the donors' common support.

There are currently two R packages for SCM analyses and one for \citeauthor{BenMichael2021}'s ``augmented'' SCM: \texttt{tidysynth}, \texttt{Synth}, \texttt{augsynth}.


\begin{figure}
\small
\caption{Heuristic tree of CSTS methods.}
    \Tree[. [.{N. treated units > 1} [.{T = 2} {Classic DiD\\ (Sec. 4.2)} ]
                           [.{T > 2} [.{Non-absorbing state\\ treatment} { {\texttt{GSC} (Sec. 4.43)}\\ {\texttt{fect} (Sec. 4.42)}\\ {\texttt{PanelMatch} (Sec. 4.43)} } ]
                                     [.{Absorbing-state\\ treatment} [.{Heterogeneity, spillovers,\\ staggered adoption} ]
                                                                      [.{Classic DiD with multiple\\ periods (Sec 4.3)} ]]]]
            [.{N. treated units = 1} [.{SCM\\ (Sec. 4.1)} ]]
          ]
\end{figure}


\subsection{Number of Treatment Periods and the Classic DiD Setup}

Whenever there are only two periods and treated units are only treated in the second period, scholars are sure to find themselves in the ``classic DiD'' setting.
This means that there are three equivalent ways to estimate the ATT: The non-parametric difference in means, the first-difference OLS and the TWFE model. These can be all estimated with base R tools and packages for linear regressions.

The three models are statistically equivalent for the estimation of ATT because the predicted outcome of the treated under control can only be predicted linearly as the pre-treatment outcome of the treated plus the trend of the control. The R code below demonstrates it with help of example data.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# Load packages}
\hlkwd{library}\hlstd{(fixest)}
\hlkwd{library}\hlstd{(tidyverse)}

\hlcom{# Filter example data (from fixest package) to only 2 periods}
\hlstd{dat_did} \hlkwb{<-} \hlstd{fixest}\hlopt{::}\hlstd{base_did} \hlopt{%>%}
    \hlkwd{filter}\hlstd{(period} \hlopt{%in%} \hlkwd{c}\hlstd{(}\hlnum{5}\hlstd{,} \hlnum{6}\hlstd{))}

\hlcom{# Create first-difference of treatment and outcome}
\hlstd{dat_did} \hlkwb{<-} \hlstd{dat_did} \hlopt{%>%}
    \hlkwd{mutate}\hlstd{(}\hlkwc{post_treat} \hlstd{= post}\hlopt{*}\hlstd{treat)} \hlopt{%>%}
    \hlkwd{group_by}\hlstd{(id)} \hlopt{%>%}
    \hlkwd{mutate}\hlstd{(}\hlkwc{y_diff} \hlstd{= y} \hlopt{-} \hlkwd{lag}\hlstd{(y),}
           \hlkwc{x_diff} \hlstd{= post_treat} \hlopt{-} \hlkwd{lag}\hlstd{(post_treat))}

\hlcom{# N. unit }
\hlkwd{length}\hlstd{(}\hlkwd{unique}\hlstd{(dat_did}\hlopt{$}\hlstd{id))}
\end{alltt}
\begin{verbatim}
## [1] 108
\end{verbatim}
\begin{alltt}
\hlcom{# N. treated units}
\hlkwd{length}\hlstd{(}\hlkwd{unique}\hlstd{(dat_did}\hlopt{$}\hlstd{id[dat_did}\hlopt{$}\hlstd{treat} \hlopt{==} \hlnum{1}\hlstd{]))}
\end{alltt}
\begin{verbatim}
## [1] 55
\end{verbatim}
\begin{alltt}
\hlcom{# Compute non-parametric DiD}
\hlstd{tau_nonp} \hlkwb{<-} \hlkwd{mean}\hlstd{(dat_did}\hlopt{$}\hlstd{y[dat_did}\hlopt{$}\hlstd{period} \hlopt{==} \hlnum{6} \hlopt{&} \hlstd{dat_did}\hlopt{$}\hlstd{treat} \hlopt{==} \hlnum{1}\hlstd{])} \hlopt{-}
    \hlkwd{mean}\hlstd{(dat_did}\hlopt{$}\hlstd{y[dat_did}\hlopt{$}\hlstd{period} \hlopt{==} \hlnum{5} \hlopt{&} \hlstd{dat_did}\hlopt{$}\hlstd{treat} \hlopt{==} \hlnum{1}\hlstd{])} \hlopt{-}
    \hlstd{(}\hlkwd{mean}\hlstd{(dat_did}\hlopt{$}\hlstd{y[dat_did}\hlopt{$}\hlstd{period} \hlopt{==} \hlnum{6} \hlopt{&} \hlstd{dat_did}\hlopt{$}\hlstd{treat} \hlopt{==} \hlnum{0}\hlstd{])} \hlopt{-}
    \hlkwd{mean}\hlstd{(dat_did}\hlopt{$}\hlstd{y[dat_did}\hlopt{$}\hlstd{period} \hlopt{==} \hlnum{5} \hlopt{&} \hlstd{dat_did}\hlopt{$}\hlstd{treat} \hlopt{==} \hlnum{0}\hlstd{]))}

\hlcom{# Fit first-difference model}
\hlstd{mod_first} \hlkwb{<-} \hlkwd{feols}\hlstd{(y_diff} \hlopt{~} \hlstd{x_diff,} \hlkwc{data} \hlstd{= dat_did)}

\hlcom{# Fit TWFE model}
\hlstd{mod_twfe} \hlkwb{<-} \hlkwd{feols}\hlstd{(y} \hlopt{~} \hlstd{post_treat} \hlopt{|} \hlstd{id} \hlopt{+} \hlstd{period,} \hlkwc{data} \hlstd{= dat_did)}

\hlcom{# Compare estimated ATT}
\hlstd{tau_nonp}
\end{alltt}
\begin{verbatim}
## [1] 1.159133
\end{verbatim}
\begin{alltt}
\hlstd{mod_first}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
## (Intercept)      x_diff 
##    1.173937    1.159133
\end{verbatim}
\begin{alltt}
\hlstd{mod_twfe}\hlopt{$}\hlstd{coefficients}
\end{alltt}
\begin{verbatim}
## post_treat 
##   1.159133
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Absorbing-state Treatments and Their Different Flavors}

In settings where the scholar observes the units over multiple periods it is important to separate the cases of absorbing state-treatments, i.e. those in which treated units remain treated after exposure, and those that do not. Next, I discuss the simplest case and the one covered by much of the econometric literature on DiD, that is absorbing-state treatment settings.

In this family of settings, the first distinction to be made concerns whether the scholar believes that the basic DiD assumptions (no effect heterogeneity, synchronous treatment, parallel trends, no anticipation) hold in a specific case. If these all hold, and the units are only observed for few extra periods before and after treatment, then in principle the TWFE approach returns the unbiased ATT. However, some of the assumptions are impossible to test and therefore it is worth running tests and alternative estimations that are robust to violations of the basic DiD setup.
In particular, multi-period DiD settings are more prone to violation of the assumptions of constant treatment effects and of no carryover effect \parencite[i.e. past treatments affect current outcomes][]{Liu}.

\subsubsection{Failure of Parallel Trends Assumption}

The parallel trend assumption is impossible to demonstrate, and even common pre-trend tests fail to really rule out the existence of differential selection bias \parencite{Roth2022}. A typical correction used by applied scholars who suspect a violation of the parallel trends assumption is to run an additional set of TWFE models with linear or quadratic time trends (i.e. interacting unit fixed effects with years and years squared). Another solution is to add a set of time-varying covariates to TWFE regressions hoping to control away the selection bias. The problem with both of these approaches is that they may fail if scholars misspecify the functional form of the time trends or covariate set.

However, currently there are various proposed solutions to this problem, which might be more or less appropriated depending on the specific application. 
One reason for the violation of the parallel trend assumption might be that the control group is too heterogeneous and hence make inappropriate comparisons. In this case, \textcite{Abadie2005,Imai2021} suggest to match units on their pre-treatment covariates to reduce heterogeneity and make the parallel trends more credible.
Another case that might undermine parallel trends is the staggered treatment adoption, whereby TWFE mechanically produce unwanted comparisons. The solutions to this problem are described below.

Finally, it might be the case that potential outcomes of treated and control observations really follow parallel trends, but only conditional on some covariates. In such cases \textcite{SantAnna2020} propose ways to estimate consistent and efficient ATTs conditional on the correct covariate specifications by reweighting units based on the propensity score. The method is implemented in the \texttt{DRDID} package.

\subsubsection{Staggered Adoption}



\subsection{Non-Absorbing Treatments and Their Estimators}

For settings that go beyond the typical policy implementation where a set of treated units receives an absorbing state treatment, methodologists have developed different approaches to extend the DiD logic of counterfactual estimation while allowing for more complicated data structures. In this section I discuss three methods: The generalized synthetic control \parencite[GSCM]{Xu2017}, the PanelMatch \parencite{Imai2021, Kim2021}, and the fect \parencite{Liu} counterfactual methods.

All three methods allow treatments to last for a certain period and be reversed, and offer generally more flexible predictions of counterfactuals than standard linear DiD methods. Moreover, they all have great R implementation and useful diagnostic tools.

\subsubsection{The Generalized Synthetic Control Method}

\textcite{Xu2017} introduces a flexible approach to estimating causal effects with CSTS data that can be thought of as a generalization of the synthetic control method, but takes inspiration from the literature on interactive fixed effects \parencite{Bai2009} and applies them to DiD settings. The GSCM computes counterfactual outcomes for the treated group by first fitting a linear model with fixed effects for time and unit interactions on the control group, then estimating unit-specific intercepts using the pre-treatment outcomes of the treated group, and finally combines the to data to predict the control outcomes for the treated units in the post-treatment period. Then the ATT is estimated as the mean difference between observed and imputed counterfactuals.

In order to make valid estimates the method needs the following assumptions: i) Strict exogeneity of one unit's error term to other units $E\left[ \varepsilon_{it}\ | \ D_{ij},x_{ij},\gamma_{ij}, f_{ij}\right] = E\left[ \varepsilon_{it}\ | \gamma_{ij}, f_{ij}\right] = 0$, ii) serial independence of one unit from other units. Moreover, the GSCM requires scholars to observe at least 10 pre-treatment periods and a minimum of 40 control units. The method is implemented through the \texttt{gsynth} R package.

\subsubsection{The \texttt{fect} Method}

\textcite{Liu} propose what can be seen as a development of GSCM because it covers a set of estimators of ATTs that fit predictive models of counterfactual outcomes on the control population. Similarly to GSCM, scholars can use one of three methods to fit models of the control group's outcomes (linear fixed effects, interactive fixed effects, and matrix completion), then use these models to infer the counterfactual outcomes under control for the treated group. 

The advantages of fect over other methods include the relaxation of the strict exogeneity assumption at least for IFE and MC estimators, which are both robust to the existence of potential time-varying confounders are captured with latent factors, and the large set of diagnostic tools that allow to test for the existence of anticipation, carryover effects, and pre-trends.

The biggest single drawback of the method is probably the novelty of most of the estimators included in the R package \texttt{fect}, which might not be received enthusiastically by most political scientists.


\subsubsection{The Panel Matching Method}

\textcite{Imai2021} develop a CSTS method that improves on existing DiD estimators by drawing on matching methods for causal inference. PanelMatch tries to solve the known problems of TWFE estimation of ATTs with a three-step procedure: 1) Scholars match treated and control units for a set of time-varying covariates as well as the outcome over a certain number of periods, 2) each treated unit's counterfactual outcome $l$ periods since treatment is estimated nonparametrically as the outcome at $l-1$ net of the average change in outcome in its matched set at the same period, 3) the period-specific ATT is computed as the average difference between observed and counterfactual outcomes for a given period since treatment, and similarly the overall ATT is the average of period-specific ATTs.

The advantages of PanelMatch are its intuitive selection of control observations, its robustness to model misspecification, the ability of using different matching and weighting procedures (Mahalanobis, propensity score, covariate-balancing propensity score), the ability to assess the matching quality before seeing effect estimates (potentially less selection bias in the model choice), and finally the fully flexible non-parametric imputation of counterfactual outcomes.

However, these advantages come at significant costs. First, PanelMatch requires the most demanding version of the parallel trends assumption, known as the ``sequential ignorability assumption,'' implying that treatment assignment at a given time must be independent from outcome, covariate and treatment histories in all previous periods. Second, because samples are explicitly restricted through the matching procedure, the estimator throws out quite some data and is therefore less efficient than alternative estimators (i.e. estimates are less precise and standard errors wider).


\section{Recap and Overview Table}


\begin{sidewaystable}
    \centering\footnotesize
    \begin{tabular}{lccccccccc}
	\toprule
	Method & Treated $N$ & Staggered & Exogeneity & Heterogeneity rob. & Treatment & Functional form & Controls & Autocorr./spillovers & Data needs \\
	\midrule
	TWFE & $N >= 1$ & No & Strict & No & Absorbing &  &  &  &  \\	
	SCM & $N = 1$ & -- & Relaxed & Yes & Absorbing &  &  &  &  \\	
	GSCM & $N >= 10$ & Yes & Relaxed & Yes & Non-Absorbing &  &  &  &  \\	
	fect & $N >= 1$ & Yes & Relaxed & Yes & Non-Absorbing &  &  &  &  \\	
	PanelMatch & $N >= 1$ & Yes & Rep. ignorability & Yes & Non-Absorbing &  &  &  &  \\	
	Two-stage DiD & $N >= 1$ & Yes & Strict & Absorbing &  &  &  &  &  \\	
	&  &  &  &  &  &  &  &  &  \\
	\bottomrule
    \end{tabular}
\end{sidewaystable}

\printbibliography

\end{document}
